MEL CREADO
COMPILADO
Model: "functional_7"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 128, 196, 1) 0                                            
__________________________________________________________________________________________________
zero_padding2d_3 (ZeroPadding2D (None, 132, 200, 1)  0           input_4[0][0]                    
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 128, 196, 10) 250         zero_padding2d_3[0][0]           
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 128, 196, 10) 0           conv2d_48[0][0]                  
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 128, 196, 10) 40          activation_45[0][0]              
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 128, 196, 12) 1080        batch_normalization_45[0][0]     
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 128, 196, 12) 0           conv2d_49[0][0]                  
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 128, 196, 12) 48          activation_46[0][0]              
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 128, 196, 14) 1512        batch_normalization_46[0][0]     
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 128, 196, 14) 0           conv2d_50[0][0]                  
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 128, 196, 14) 56          activation_47[0][0]              
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 128, 196, 15) 1890        batch_normalization_47[0][0]     
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 128, 196, 15) 0           conv2d_51[0][0]                  
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 128, 196, 15) 60          activation_48[0][0]              
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 128, 196, 19) 2565        batch_normalization_48[0][0]     
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 128, 196, 19) 0           conv2d_52[0][0]                  
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 128, 196, 19) 76          activation_49[0][0]              
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 128, 196, 21) 3591        batch_normalization_49[0][0]     
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 128, 196, 21) 0           conv2d_53[0][0]                  
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 128, 196, 21) 84          activation_50[0][0]              
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 128, 196, 23) 4347        batch_normalization_50[0][0]     
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 128, 196, 23) 0           conv2d_54[0][0]                  
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 128, 196, 23) 92          activation_51[0][0]              
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 128, 196, 25) 5175        batch_normalization_51[0][0]     
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 128, 196, 25) 0           conv2d_55[0][0]                  
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 128, 196, 25) 100         activation_52[0][0]              
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 128, 196, 23) 5175        batch_normalization_52[0][0]     
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 128, 196, 23) 0           conv2d_56[0][0]                  
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 128, 196, 23) 92          activation_53[0][0]              
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 128, 196, 21) 4347        batch_normalization_53[0][0]     
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 128, 196, 21) 0           conv2d_57[0][0]                  
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 128, 196, 21) 84          activation_54[0][0]              
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 128, 196, 19) 3591        batch_normalization_54[0][0]     
__________________________________________________________________________________________________
tf_op_layer_AddV2_6 (TensorFlow [(None, 128, 196, 19 0           conv2d_58[0][0]                  
                                                                 conv2d_52[0][0]                  
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 128, 196, 19) 0           tf_op_layer_AddV2_6[0][0]        
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 128, 196, 19) 76          activation_55[0][0]              
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 128, 196, 15) 2565        batch_normalization_55[0][0]     
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 128, 196, 15) 0           conv2d_59[0][0]                  
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 128, 196, 15) 60          activation_56[0][0]              
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 128, 196, 14) 1890        batch_normalization_56[0][0]     
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 128, 196, 14) 0           conv2d_60[0][0]                  
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 128, 196, 14) 56          activation_57[0][0]              
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 128, 196, 12) 1512        batch_normalization_57[0][0]     
__________________________________________________________________________________________________
tf_op_layer_AddV2_7 (TensorFlow [(None, 128, 196, 12 0           conv2d_61[0][0]                  
                                                                 conv2d_49[0][0]                  
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 128, 196, 12) 0           tf_op_layer_AddV2_7[0][0]        
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 128, 196, 12) 48          activation_58[0][0]              
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 128, 196, 10) 3000        batch_normalization_58[0][0]     
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 128, 196, 10) 0           conv2d_62[0][0]                  
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 128, 196, 10) 40          activation_59[0][0]              
__________________________________________________________________________________________________
spatial_dropout2d_3 (SpatialDro (None, 128, 196, 10) 0           batch_normalization_59[0][0]     
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 128, 196, 1)  91          spatial_dropout2d_3[0][0]        
==================================================================================================
Total params: 43,593
Trainable params: 43,087
Non-trainable params: 506
__________________________________________________________________________________________________
FIT START
Epoch 1/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0793 - mae: 0.1916 WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0070s). Check your callbacks.
1201/1201 [==============================] - 44s 37ms/step - loss: 0.0793 - mae: 0.1916 - val_loss: 0.0526 - val_mae: 0.1920
Epoch 2/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0341 - mae: 0.1319 - val_loss: 0.0316 - val_mae: 0.1231
Epoch 3/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0323 - mae: 0.1261 - val_loss: 0.0371 - val_mae: 0.1450
Epoch 4/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0309 - mae: 0.1222 - val_loss: 0.1523 - val_mae: 0.3389
Epoch 5/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0301 - mae: 0.1202 - val_loss: 0.0346 - val_mae: 0.1241
Epoch 6/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0296 - mae: 0.1190 
Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0296 - mae: 0.1190 - val_loss: 0.0392 - val_mae: 0.1374
Epoch 7/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0292 - mae: 0.1182 - val_loss: 0.0294 - val_mae: 0.1191
Epoch 8/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0286 - mae: 0.1172 - val_loss: 0.0337 - val_mae: 0.1385
Epoch 9/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0282 - mae: 0.1168 - val_loss: 0.0274 - val_mae: 0.1143
Epoch 10/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0280 - mae: 0.1166 - val_loss: 0.0288 - val_mae: 0.1185
Epoch 11/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0280 - mae: 0.1168 - val_loss: 0.0278 - val_mae: 0.1147
Epoch 12/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0277 - mae: 0.1161 - val_loss: 0.0272 - val_mae: 0.1166
Epoch 13/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0276 - mae: 0.1162 - val_loss: 0.0273 - val_mae: 0.1147
Epoch 14/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0276 - mae: 0.1163 - val_loss: 0.0271 - val_mae: 0.1145
Epoch 15/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0276 - mae: 0.1162 - val_loss: 0.0402 - val_mae: 0.1654
Epoch 16/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0274 - mae: 0.1160 - val_loss: 0.0289 - val_mae: 0.1182
Epoch 17/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0274 - mae: 0.1159 - val_loss: 0.0271 - val_mae: 0.1135
Epoch 18/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0273 - mae: 0.1158 
Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0273 - mae: 0.1158 - val_loss: 0.0272 - val_mae: 0.1140
Epoch 19/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0270 - mae: 0.1151 - val_loss: 0.0265 - val_mae: 0.1138
Epoch 20/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0270 - mae: 0.1153 - val_loss: 0.0265 - val_mae: 0.1140
Epoch 21/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0269 - mae: 0.1152 - val_loss: 0.0267 - val_mae: 0.1136
Epoch 22/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0268 - mae: 0.1149 - val_loss: 0.0266 - val_mae: 0.1129
Epoch 23/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0267 - mae: 0.1147 
Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0267 - mae: 0.1147 - val_loss: 0.0290 - val_mae: 0.1229
Epoch 24/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0268 - mae: 0.1147 - val_loss: 0.0264 - val_mae: 0.1147
Epoch 25/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0267 - mae: 0.1149 - val_loss: 0.0262 - val_mae: 0.1136
Epoch 26/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0266 - mae: 0.1147 - val_loss: 0.0263 - val_mae: 0.1133
Epoch 27/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0265 - mae: 0.1145 - val_loss: 0.0263 - val_mae: 0.1138
Epoch 28/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0266 - mae: 0.1147 - val_loss: 0.0263 - val_mae: 0.1136
Epoch 29/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0265 - mae: 0.1144 
Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.375000081490725e-05.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0265 - mae: 0.1144 - val_loss: 0.0267 - val_mae: 0.1151
Epoch 30/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0264 - mae: 0.1145 - val_loss: 0.0262 - val_mae: 0.1146
Epoch 31/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0265 - mae: 0.1144 - val_loss: 0.0261 - val_mae: 0.1130
Epoch 32/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0263 - mae: 0.1142 - val_loss: 0.0261 - val_mae: 0.1131
Epoch 33/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0264 - mae: 0.1144 - val_loss: 0.0261 - val_mae: 0.1139
Epoch 34/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0263 - mae: 0.1141 - val_loss: 0.0262 - val_mae: 0.1132
Epoch 35/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0263 - mae: 0.1142 
Epoch 00035: ReduceLROnPlateau reducing learning rate to 4.6875000407453626e-05.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0263 - mae: 0.1142 - val_loss: 0.0261 - val_mae: 0.1133
Epoch 36/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0264 - mae: 0.1141 - val_loss: 0.0260 - val_mae: 0.1126
Epoch 37/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1142 - val_loss: 0.0261 - val_mae: 0.1139
Epoch 38/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0263 - mae: 0.1141 - val_loss: 0.0261 - val_mae: 0.1131
Epoch 39/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1138 - val_loss: 0.0260 - val_mae: 0.1124
Epoch 40/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1138 - val_loss: 0.0261 - val_mae: 0.1138
Epoch 41/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1141 - val_loss: 0.0261 - val_mae: 0.1122
Epoch 42/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1140 - val_loss: 0.0260 - val_mae: 0.1134
Epoch 43/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0263 - mae: 0.1141 
Epoch 00043: ReduceLROnPlateau reducing learning rate to 2.3437500203726813e-05.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0263 - mae: 0.1141 - val_loss: 0.0261 - val_mae: 0.1130
Epoch 44/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1140 - val_loss: 0.0260 - val_mae: 0.1128
Epoch 45/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1140 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 46/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1138 - val_loss: 0.0260 - val_mae: 0.1126
Epoch 47/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0261 - mae: 0.1137 
Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.1718750101863407e-05.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1138
Epoch 48/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1131
Epoch 49/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1139 - val_loss: 0.0260 - val_mae: 0.1134
Epoch 50/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1138 - val_loss: 0.0260 - val_mae: 0.1126
Epoch 51/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0261 - mae: 0.1137 
Epoch 00051: ReduceLROnPlateau reducing learning rate to 5.859375050931703e-06.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 52/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1138 - val_loss: 0.0260 - val_mae: 0.1131
Epoch 53/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1140 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 54/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1139 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 55/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0262 - mae: 0.1138 
Epoch 00055: ReduceLROnPlateau reducing learning rate to 2.9296875254658516e-06.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1138 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 56/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0259 - val_mae: 0.1129
Epoch 57/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1139 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 58/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1132
Epoch 59/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0260 - mae: 0.1135 
Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.4648437627329258e-06.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1135 - val_loss: 0.0260 - val_mae: 0.1131
Epoch 60/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1139 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 61/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1135 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 62/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1138 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 63/10000
1200/1201 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.1137 
Epoch 00063: ReduceLROnPlateau reducing learning rate to 7.324218813664629e-07.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1131
Epoch 64/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0259 - val_mae: 0.1130
Epoch 65/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 66/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 67/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0260 - mae: 0.1135 
Epoch 00067: ReduceLROnPlateau reducing learning rate to 3.6621094068323146e-07.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1135 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 68/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 69/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 70/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 71/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0261 - mae: 0.1137 
Epoch 00071: ReduceLROnPlateau reducing learning rate to 1.8310547034161573e-07.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 72/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1138 - val_loss: 0.0260 - val_mae: 0.1131
Epoch 73/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1135 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 74/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 75/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0261 - mae: 0.1137 
Epoch 00075: ReduceLROnPlateau reducing learning rate to 9.155273517080786e-08.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1131
Epoch 76/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 77/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 78/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1138 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 79/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0260 - mae: 0.1136 
Epoch 00079: ReduceLROnPlateau reducing learning rate to 4.577636758540393e-08.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1128
Epoch 80/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1139 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 81/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1134 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 82/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 83/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0262 - mae: 0.1137 
Epoch 00083: ReduceLROnPlateau reducing learning rate to 2.2888183792701966e-08.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 84/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 85/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1131
Epoch 86/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 87/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0260 - mae: 0.1136 
Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.1444091896350983e-08.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 88/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1131
Epoch 89/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1134 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 90/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 91/10000
1201/1201 [==============================] - ETA: 0s - loss: 0.0262 - mae: 0.1140 
Epoch 00091: ReduceLROnPlateau reducing learning rate to 1e-08.
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1140 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 92/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1135 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 93/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 94/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 95/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 96/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 97/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 98/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1139 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 99/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1139 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 100/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1134 - val_loss: 0.0260 - val_mae: 0.1128
Epoch 101/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 102/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1128
Epoch 103/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 104/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 105/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1134 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 106/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 107/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1138 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 108/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 109/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1139 - val_loss: 0.0259 - val_mae: 0.1130
Epoch 110/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1135 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 111/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1139 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 112/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1138 - val_loss: 0.0260 - val_mae: 0.1131
Epoch 113/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1139 - val_loss: 0.0259 - val_mae: 0.1130
Epoch 114/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1139 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 115/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0259 - mae: 0.1132 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 116/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 117/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 118/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 119/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1138 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 120/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1138 - val_loss: 0.0260 - val_mae: 0.1131
Epoch 121/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1135 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 122/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 123/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1139 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 124/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1140 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 125/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 126/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1136 - val_loss: 0.0259 - val_mae: 0.1130
Epoch 127/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 128/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1135 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 129/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 130/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1135 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 131/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0262 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 132/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1138 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 133/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1135 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 134/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1139 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 135/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1135 - val_loss: 0.0259 - val_mae: 0.1130
Epoch 136/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1134 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 137/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1139 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 138/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1130
Epoch 139/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0260 - mae: 0.1136 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 140/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1129
Epoch 141/10000
1201/1201 [==============================] - 43s 36ms/step - loss: 0.0261 - mae: 0.1137 - val_loss: 0.0260 - val_mae: 0.1130
FIT END



Las figuras ahora se renderizan en el panel de Gráficos por defecto. Para que también aparezcan en la terminal, desactive “Silenciar los gráficos en línea” en el menú de opciones del panel de Gráficos. 

  1/265 [..............................] - ETA: 1s - loss: 0.0149 - mae: 0.0787WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0060s). Check your callbacks.
265/265 [==============================] - 2s 8ms/step - loss: 0.0261 - mae: 0.1122
Test loss:0.026065
Test mae:0.112197
C:\Users\Paco\anaconda3\lib\site-packages\librosa\core\audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.
  warnings.warn('PySoundFile failed. Trying audioread instead.')
PREDICIENDO..
MELAUDIO..
WAV
END